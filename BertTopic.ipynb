{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from BertForSequenceClassificationOutputPooled import *\n",
    "from BertTM import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models, losses\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', )\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained('bert-base-uncased', \n",
    "                                                              output_attentions=True, \n",
    "                                                              output_hidden_states=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../bert-classifier-pytorch/model_save_attention_1epoch\"\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained(output_dir,\n",
    "                                                      output_attentions = True, \n",
    "                                                      output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.BERT(output_dir, max_seq_length = 240,)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "st_model = SentenceTransformer(modules=[word_embedding_model, pooling_model],\n",
    "                               #device=torch.device(\"cuda\")\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that attention and vectorization work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = get_attention(sentences, model, tokenizer, method = 'first')\n",
    "np.sum([tpl[1] for tpl in attentions[1]])\n",
    "\n",
    "vectorized = vectorize(sentences, model, tokenizer)\n",
    "torch.stack(vectorized).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 0.0613682),\n",
       "  ('movie', 0.030429687),\n",
       "  ('was', 0.035641603),\n",
       "  ('the', 0.21911351),\n",
       "  ('cutest', 0.06270852),\n",
       "  ('.', 0.09440403),\n",
       "  ('read', 0.0423442),\n",
       "  ('more', 0.06480052),\n",
       "  ('at', 0.06262489),\n",
       "  ('http', 0.027938599),\n",
       "  (':', 0.043154325),\n",
       "  ('/', 0.038011733),\n",
       "  ('/', 0.041409045),\n",
       "  ('worstever', 0.04187157),\n",
       "  ('.', 0.08471608),\n",
       "  ('com', 0.049463503)]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention([\"this movie was the cutest. read more at http://worstever.com\"], model, tokenizer, method = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nlwx_2020_hashtags_no_rt_predictions.csv\")\n",
    "#df = preprocess(df)\n",
    "data = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "ngram = (1, 3)\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows out of 100.\n"
     ]
    }
   ],
   "source": [
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(data), batch_size):\n",
    "    index = min(i + batch_size, len(data))\n",
    "    rows.append(vectorize(data[i:index], model, tokenizer))\n",
    "    attentions.extend(get_attention(data[i:index], model, tokenizer))\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(data)}.\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows in 0.15 seconds.\n",
      "Processed 500 rows in 98.27 seconds.\n",
      "Processed 1000 rows in 193.77 seconds.\n",
      "Processed 1500 rows in 290.81 seconds.\n",
      "Processed 2000 rows in 386.74 seconds.\n",
      "Processed 2500 rows in 484.5 seconds.\n",
      "Processed 3000 rows in 584.24 seconds.\n",
      "Processed 3500 rows in 679.84 seconds.\n",
      "Processed 4000 rows in 776.88 seconds.\n",
      "Processed 4500 rows in 873.04 seconds.\n",
      "Processed 5000 rows in 969.55 seconds.\n",
      "Processed 5500 rows in 1067.45 seconds.\n",
      "Processed 6000 rows in 1168.92 seconds.\n",
      "Processed 6500 rows in 1268.35 seconds.\n",
      "Processed 7000 rows in 1366.79 seconds.\n",
      "Processed 7500 rows in 1468.06 seconds.\n",
      "Processed 8000 rows in 1570.23 seconds.\n",
      "Processed 8500 rows in 1671.15 seconds.\n",
      "Processed 9000 rows in 1772.01 seconds.\n",
      "Processed 9500 rows in 1872.32 seconds.\n",
      "Processed 10000 rows in 1970.01 seconds.\n",
      "Processed 10500 rows in 2071.37 seconds.\n",
      "Processed 11000 rows in 2172.99 seconds.\n",
      "Processed 11500 rows in 2273.16 seconds.\n",
      "Processed 12000 rows in 2370.85 seconds.\n",
      "Processed 12500 rows in 2469.11 seconds.\n",
      "Processed 13000 rows in 2567.32 seconds.\n",
      "Processed 13500 rows in 2668.98 seconds.\n",
      "Processed 14000 rows in 2772.43 seconds.\n",
      "Processed 14500 rows in 2876.16 seconds.\n",
      "Processed 15000 rows in 2975.96 seconds.\n",
      "Processed 15500 rows in 3077.56 seconds.\n",
      "Processed 16000 rows in 3179.27 seconds.\n",
      "Processed 16500 rows in 3279.89 seconds.\n",
      "Processed 17000 rows in 3381.93 seconds.\n",
      "Processed 17500 rows in 3483.49 seconds.\n",
      "Processed 18000 rows in 3584.39 seconds.\n",
      "Processed 18500 rows in 3688.12 seconds.\n",
      "Processed 19000 rows in 3790.6 seconds.\n",
      "Processed 19500 rows in 3891.36 seconds.\n",
      "Processed 20000 rows in 3987.45 seconds.\n",
      "Processed 20500 rows in 4084.35 seconds.\n",
      "Processed 21000 rows in 4185.52 seconds.\n",
      "Processed 21500 rows in 4286.16 seconds.\n",
      "CPU times: user 1h 11min 51s, sys: 28.7 s, total: 1h 12min 20s\n",
      "Wall time: 1h 12min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rows, attentions = [], []\n",
    "start_time = time.time()\n",
    "for i in range(0, len(data)):\n",
    "    rows.extend(st_model.encode([data[i]]))\n",
    "    attentions.extend(get_attention([data[i]], model, tokenizer))\n",
    "    if i % 500 == 0:\n",
    "        print(f'Processed {(i)} rows in {round(time.time() - start_time, 2)} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1323\n",
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10']\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords-en.json') as fopen:\n",
    "    stopwords = json.load(fopen)\n",
    "\n",
    "stopwords.extend(['#', '@', '…', \"'\", \"’\", \"[UNK]\", \"\\\"\", \";\", \"*\", \"_\", \"amp\", \"&\",\n",
    "                 'nlwhiteout', 'nlweather', 'newfoundland', 'nlblizzard2020', 'nlstorm2020',\n",
    "                  'snowmaggedon2020', 'stormageddon2020', 'snowpocalypse2020', 'snowmageddon',\n",
    "                  'nlstorm', 'nltraffic', 'nlwx', 'nlblizzard'])\n",
    "    \n",
    "print(len(stopwords))\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat = np.concatenate(rows, axis = 0)\n",
    "#concat = [item.detach().numpy() for item in concat]\n",
    "#concat = np.asarray(concat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = []\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    all_model_data.append((data[i], df.prediction[i], attentions[i], rows[i]))\n",
    "    \n",
    "#pickle.dump(all_model_data, open(f\"attentions_sent_embeddings.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = pickle.load(open(\"attentions_sent_embeddings.pkl\", \"rb\"))\n",
    "texts, _, attentions, rows = zip(*all_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting kmeans model.\n",
      "CPU times: user 6.34 s, sys: 840 ms, total: 7.18 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Fitting kmeans model.\")\n",
    "rows = rows[:5000]\n",
    "texts = texts[:5000]\n",
    "attentions = attentions[:5000]\n",
    "kmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(rows)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering attentions.\n",
      "CPU times: user 4.19 s, sys: 40 ms, total: 4.23 s\n",
      "Wall time: 4.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "overall, filtered_a, filtered_texts, filtered_l = [], [], [], []\n",
    "url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "print(\"Filtering attentions.\")\n",
    "for idx, a in enumerate(attentions):\n",
    "    f = [(Word(i[0]).lemmatize(),i[1]) for i in a if i[0] not in stopwords and i[0] not in url_re]\n",
    "    f_txt = [w[0] for w in f]\n",
    "    if len(f) > 0:\n",
    "        overall.extend(f)\n",
    "        filtered_a.append(f)\n",
    "        filtered_texts.append(f_txt)\n",
    "        filtered_l.append(labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ngrams.\n",
      "CPU times: user 120 ms, sys: 0 ns, total: 120 ms\n",
      "Wall time: 120 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Generating ngrams.\")\n",
    "o_ngram = generate_ngram(overall, ngram)\n",
    "features = []\n",
    "for i in o_ngram:\n",
    "    features.append(' '.join([w[0] for w in i]))\n",
    "features = list(set(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining cluster components. This will take awhile. \n",
      "Progress will be printed for every 500th processed property.\n",
      "\n",
      "Processed 500 texts in 8.88 seconds.\n",
      "Processed 1000 texts in 17.97 seconds.\n",
      "Processed 1500 texts in 29.17 seconds.\n",
      "Processed 2000 texts in 38.65 seconds.\n",
      "Processed 2500 texts in 48.81 seconds.\n",
      "Processed 3000 texts in 58.62 seconds.\n",
      "Processed 3500 texts in 68.35 seconds.\n",
      "Processed 4000 texts in 78.25 seconds.\n",
      "Processed 4500 texts in 89.35 seconds.\n",
      "Finished determining a total of 4989 cluster components. Total time 99.83 seconds.\n",
      "CPU times: user 1min 39s, sys: 20 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "Determining cluster components. This will take awhile. \n",
    "Progress will be printed for every 500th processed property.\n",
    "\"\"\")\n",
    "\n",
    "components = {}\n",
    "words_label = {}\n",
    "start_time = time.time()\n",
    "for idx, label in enumerate(filtered_l):\n",
    "    if label not in components:\n",
    "        components[label] = {}\n",
    "        words_label[label] = []\n",
    "    else:\n",
    "        f = generate_ngram(filtered_a[idx], ngram)\n",
    "        for w in f:\n",
    "            word = ' '.join([r[0] for r in w])\n",
    "            score = np.mean([r[1] for r in w])\n",
    "            if word in features:\n",
    "                if word in components[label]:\n",
    "                    components[label][word] += score\n",
    "                else:\n",
    "                    components[label][word] = score\n",
    "                words_label[label].append(word)\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f'Processed {(idx + 1)} texts in {round(time.time() - start_time, 2)} seconds.')\n",
    "            \n",
    "print(f\"Finished determining a total of {idx + 1} cluster components. Total time {round(time.time() - start_time, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_indexed = tf_icf(words_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(components, open(\"components.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_tfidf_attn = {}\n",
    "components_tfidf = {}\n",
    "for k1 in components:\n",
    "    components_tfidf_attn[k1] = {}\n",
    "    components_tfidf[k1] = {}\n",
    "    for k2 in components[k1]:\n",
    "        components_tfidf_attn[k1][k2] = fully_indexed[k1][k2] * components[k1][k2]\n",
    "        components_tfidf[k1][k2] = fully_indexed[k1][k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_attn = topics_df(\n",
    "    10,\n",
    "    components,\n",
    "    n_words = 10)\n",
    "\n",
    "#pickle.dump(topics_attn, open(\"topics_sent_embed.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>view</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>hope</td>\n",
       "      <td>stay</td>\n",
       "      <td>outage</td>\n",
       "      <td>snow</td>\n",
       "      <td>storm</td>\n",
       "      <td>storm</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window</td>\n",
       "      <td>closed</td>\n",
       "      <td>wind</td>\n",
       "      <td>stay</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>power</td>\n",
       "      <td>storm</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>view window</td>\n",
       "      <td>snow</td>\n",
       "      <td>emergency</td>\n",
       "      <td>safe</td>\n",
       "      <td>safe</td>\n",
       "      <td>power outage</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>snow</td>\n",
       "      <td>wind</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paradise</td>\n",
       "      <td>emergency</td>\n",
       "      <td>storm</td>\n",
       "      <td>thinking</td>\n",
       "      <td>storm</td>\n",
       "      <td>closed</td>\n",
       "      <td>snowday</td>\n",
       "      <td>stay</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>ken starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>door</td>\n",
       "      <td>road</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>warning</td>\n",
       "      <td>close</td>\n",
       "      <td>winter</td>\n",
       "      <td>buried</td>\n",
       "      <td>weather</td>\n",
       "      <td>ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>storm</td>\n",
       "      <td>storm</td>\n",
       "      <td>snowfall</td>\n",
       "      <td>hoping</td>\n",
       "      <td>friend</td>\n",
       "      <td>business close</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>house</td>\n",
       "      <td>emergency</td>\n",
       "      <td>saveng ken starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wow</td>\n",
       "      <td>stay</td>\n",
       "      <td>john</td>\n",
       "      <td>friend</td>\n",
       "      <td>warm</td>\n",
       "      <td>lost</td>\n",
       "      <td>wind</td>\n",
       "      <td>missing</td>\n",
       "      <td>stormchips</td>\n",
       "      <td>eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>breaking</td>\n",
       "      <td>plow</td>\n",
       "      <td>declared</td>\n",
       "      <td>storm</td>\n",
       "      <td>stay safe warm</td>\n",
       "      <td>emergency</td>\n",
       "      <td>weather</td>\n",
       "      <td>shovel</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>eminem saveng ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>car</td>\n",
       "      <td>weather</td>\n",
       "      <td>gust</td>\n",
       "      <td>prayer</td>\n",
       "      <td>snow</td>\n",
       "      <td>emergency business close</td>\n",
       "      <td>day</td>\n",
       "      <td>stuck</td>\n",
       "      <td>monster</td>\n",
       "      <td>saveng ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>road</td>\n",
       "      <td>lost</td>\n",
       "      <td>airport</td>\n",
       "      <td>hope stay</td>\n",
       "      <td>emergency</td>\n",
       "      <td>business</td>\n",
       "      <td>view</td>\n",
       "      <td>car</td>\n",
       "      <td>day</td>\n",
       "      <td>ken starr sexeducation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic 0    topic 1    topic 2    topic 3         topic 4  \\\n",
       "0         view      power       snow       hope            stay   \n",
       "1       window     closed       wind       stay       stay safe   \n",
       "2  view window       snow  emergency       safe            safe   \n",
       "3     paradise  emergency      storm   thinking           storm   \n",
       "4         door       road   blizzard  stay safe         warning   \n",
       "5        storm      storm   snowfall     hoping          friend   \n",
       "6          wow       stay       john     friend            warm   \n",
       "7     breaking       plow   declared      storm  stay safe warm   \n",
       "8          car    weather       gust     prayer            snow   \n",
       "9         road       lost    airport  hope stay       emergency   \n",
       "\n",
       "                    topic 5    topic 6  topic 7     topic 8  \\\n",
       "0                    outage       snow    storm       storm   \n",
       "1                     power      storm    power        snow   \n",
       "2              power outage   blizzard     snow        wind   \n",
       "3                    closed    snowday     stay    blizzard   \n",
       "4                     close     winter   buried     weather   \n",
       "5            business close  snowstorm    house   emergency   \n",
       "6                      lost       wind  missing  stormchips   \n",
       "7                 emergency    weather   shovel   snowstorm   \n",
       "8  emergency business close        day    stuck     monster   \n",
       "9                  business       view      car         day   \n",
       "\n",
       "                  topic 9  \n",
       "0                   storm  \n",
       "1                   starr  \n",
       "2                    snow  \n",
       "3               ken starr  \n",
       "4                     ken  \n",
       "5        saveng ken starr  \n",
       "6                  eminem  \n",
       "7       eminem saveng ken  \n",
       "8              saveng ken  \n",
       "9  ken starr sexeducation  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>view</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>safe</td>\n",
       "      <td>safe</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window</td>\n",
       "      <td>snow</td>\n",
       "      <td>winds</td>\n",
       "      <td>hope</td>\n",
       "      <td>stay</td>\n",
       "      <td>outages</td>\n",
       "      <td>storm</td>\n",
       "      <td>snow</td>\n",
       "      <td>storm</td>\n",
       "      <td>eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>view window</td>\n",
       "      <td>closed</td>\n",
       "      <td>john</td>\n",
       "      <td>stay</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>businesses close</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>storm</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>sexeducation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sharing view</td>\n",
       "      <td>emergency</td>\n",
       "      <td>storm</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>storm</td>\n",
       "      <td>power outages</td>\n",
       "      <td>snowday</td>\n",
       "      <td>house</td>\n",
       "      <td>wind</td>\n",
       "      <td>ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sharing view window</td>\n",
       "      <td>roads</td>\n",
       "      <td>emergency</td>\n",
       "      <td>thinking</td>\n",
       "      <td>safe stay</td>\n",
       "      <td>emergency businesses close</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "      <td>car</td>\n",
       "      <td>weather</td>\n",
       "      <td>starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>onthegocbc</td>\n",
       "      <td>john</td>\n",
       "      <td>airport</td>\n",
       "      <td>friends</td>\n",
       "      <td>emergency</td>\n",
       "      <td>businesses</td>\n",
       "      <td>winter</td>\n",
       "      <td>window</td>\n",
       "      <td>winds</td>\n",
       "      <td>даниила</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sharing</td>\n",
       "      <td>update</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>prayers</td>\n",
       "      <td>warm</td>\n",
       "      <td>outage</td>\n",
       "      <td>day</td>\n",
       "      <td>stay</td>\n",
       "      <td>john</td>\n",
       "      <td>егорова</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cbcnl</td>\n",
       "      <td>road</td>\n",
       "      <td>declared</td>\n",
       "      <td>family</td>\n",
       "      <td>stay safe warm</td>\n",
       "      <td>declared emergency</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>door</td>\n",
       "      <td>day</td>\n",
       "      <td>photo learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>john</td>\n",
       "      <td>conditions</td>\n",
       "      <td>snowfall</td>\n",
       "      <td>hoping</td>\n",
       "      <td>snow</td>\n",
       "      <td>emergency businesses</td>\n",
       "      <td>view</td>\n",
       "      <td>hospital</td>\n",
       "      <td>snowiest</td>\n",
       "      <td>learn michelleobama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>door</td>\n",
       "      <td>storm</td>\n",
       "      <td>wind</td>\n",
       "      <td>warm</td>\n",
       "      <td>safe warm</td>\n",
       "      <td>declared emergency businesses</td>\n",
       "      <td>window</td>\n",
       "      <td>john</td>\n",
       "      <td>emergency</td>\n",
       "      <td>michelleobama eminem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               topic 0     topic 1    topic 2    topic 3         topic 4  \\\n",
       "0                 view       power       snow       safe            safe   \n",
       "1               window        snow      winds       hope            stay   \n",
       "2          view window      closed       john       stay       stay safe   \n",
       "3         sharing view   emergency      storm  stay safe           storm   \n",
       "4  sharing view window       roads  emergency   thinking       safe stay   \n",
       "5           onthegocbc        john    airport    friends       emergency   \n",
       "6              sharing      update   blizzard    prayers            warm   \n",
       "7                cbcnl        road   declared     family  stay safe warm   \n",
       "8                 john  conditions   snowfall     hoping            snow   \n",
       "9                 door       storm       wind       warm       safe warm   \n",
       "\n",
       "                         topic 5           topic 6   topic 7    topic 8  \\\n",
       "0                          power              snow     power       snow   \n",
       "1                        outages             storm      snow      storm   \n",
       "2               businesses close          blizzard     storm   blizzard   \n",
       "3                  power outages           snowday     house       wind   \n",
       "4     emergency businesses close  snowmageddon2020       car    weather   \n",
       "5                     businesses            winter    window      winds   \n",
       "6                         outage               day      stay       john   \n",
       "7             declared emergency         snowstorm      door        day   \n",
       "8           emergency businesses              view  hospital   snowiest   \n",
       "9  declared emergency businesses            window      john  emergency   \n",
       "\n",
       "                topic 9  \n",
       "0                 learn  \n",
       "1                eminem  \n",
       "2          sexeducation  \n",
       "3                   ken  \n",
       "4                 starr  \n",
       "5               даниила  \n",
       "6               егорова  \n",
       "7           photo learn  \n",
       "8   learn michelleobama  \n",
       "9  michelleobama eminem  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf = topics_df(\n",
    "    10,\n",
    "    components_tfidf,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>view</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>safe</td>\n",
       "      <td>stay</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>window</td>\n",
       "      <td>closed</td>\n",
       "      <td>storm</td>\n",
       "      <td>hope</td>\n",
       "      <td>safe</td>\n",
       "      <td>outages</td>\n",
       "      <td>storm</td>\n",
       "      <td>snow</td>\n",
       "      <td>storm</td>\n",
       "      <td>ken starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>view window</td>\n",
       "      <td>snow</td>\n",
       "      <td>winds</td>\n",
       "      <td>stay</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>outage</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>storm</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sharing view window</td>\n",
       "      <td>emergency</td>\n",
       "      <td>emergency</td>\n",
       "      <td>thinking</td>\n",
       "      <td>storm</td>\n",
       "      <td>power outages</td>\n",
       "      <td>snowday</td>\n",
       "      <td>stay</td>\n",
       "      <td>wind</td>\n",
       "      <td>eminem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sharing view</td>\n",
       "      <td>roads</td>\n",
       "      <td>john</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>warm</td>\n",
       "      <td>businesses close</td>\n",
       "      <td>winter</td>\n",
       "      <td>house</td>\n",
       "      <td>weather</td>\n",
       "      <td>saveng ken starr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>door</td>\n",
       "      <td>storm</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>friends</td>\n",
       "      <td>stay safe warm</td>\n",
       "      <td>close</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>buried</td>\n",
       "      <td>winds</td>\n",
       "      <td>eminem saveng ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>john</td>\n",
       "      <td>weather</td>\n",
       "      <td>snowfall</td>\n",
       "      <td>hoping</td>\n",
       "      <td>emergency</td>\n",
       "      <td>closed</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "      <td>car</td>\n",
       "      <td>emergency</td>\n",
       "      <td>saveng ken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>paradise</td>\n",
       "      <td>road</td>\n",
       "      <td>wind</td>\n",
       "      <td>prayers</td>\n",
       "      <td>safe stay</td>\n",
       "      <td>power outage</td>\n",
       "      <td>day</td>\n",
       "      <td>shovel</td>\n",
       "      <td>stormchips</td>\n",
       "      <td>ken starr sexeducation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>car</td>\n",
       "      <td>clearing</td>\n",
       "      <td>declared</td>\n",
       "      <td>storm</td>\n",
       "      <td>friends</td>\n",
       "      <td>emergency businesses close</td>\n",
       "      <td>weather</td>\n",
       "      <td>stuck</td>\n",
       "      <td>day</td>\n",
       "      <td>eminem saveng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>road</td>\n",
       "      <td>conditions</td>\n",
       "      <td>airport</td>\n",
       "      <td>family</td>\n",
       "      <td>snow</td>\n",
       "      <td>emergency</td>\n",
       "      <td>wind</td>\n",
       "      <td>emergency</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>starr sexeducation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               topic 0     topic 1    topic 2    topic 3         topic 4  \\\n",
       "0                 view       power       snow       safe            stay   \n",
       "1               window      closed      storm       hope            safe   \n",
       "2          view window        snow      winds       stay       stay safe   \n",
       "3  sharing view window   emergency  emergency   thinking           storm   \n",
       "4         sharing view       roads       john  stay safe            warm   \n",
       "5                 door       storm   blizzard    friends  stay safe warm   \n",
       "6                 john     weather   snowfall     hoping       emergency   \n",
       "7             paradise        road       wind    prayers       safe stay   \n",
       "8                  car    clearing   declared      storm         friends   \n",
       "9                 road  conditions    airport     family            snow   \n",
       "\n",
       "                      topic 5           topic 6    topic 7     topic 8  \\\n",
       "0                       power              snow      power        snow   \n",
       "1                     outages             storm       snow       storm   \n",
       "2                      outage          blizzard      storm    blizzard   \n",
       "3               power outages           snowday       stay        wind   \n",
       "4            businesses close            winter      house     weather   \n",
       "5                       close         snowstorm     buried       winds   \n",
       "6                      closed  snowmageddon2020        car   emergency   \n",
       "7                power outage               day     shovel  stormchips   \n",
       "8  emergency businesses close           weather      stuck         day   \n",
       "9                   emergency              wind  emergency   snowstorm   \n",
       "\n",
       "                  topic 9  \n",
       "0                   starr  \n",
       "1               ken starr  \n",
       "2                     ken  \n",
       "3                  eminem  \n",
       "4        saveng ken starr  \n",
       "5       eminem saveng ken  \n",
       "6              saveng ken  \n",
       "7  ken starr sexeducation  \n",
       "8           eminem saveng  \n",
       "9      starr sexeducation  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf_attn = topics_df(\n",
    "    10,\n",
    "    components_tfidf_attn,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allen NLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
