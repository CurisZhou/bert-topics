{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from BertForSequenceClassificationOutputPooled import *\n",
    "from BertTM import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models, losses\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained('bert-base-uncased', \n",
    "                                                              output_attentions=True, \n",
    "                                                              output_hidden_states=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../bert-classifier-pytorch/model_save_attention_1epoch\"\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained(output_dir,\n",
    "                                                      output_attentions = True, \n",
    "                                                      output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.BERT(output_dir, max_seq_length = 240,)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "st_model = SentenceTransformer(modules=[word_embedding_model, pooling_model],\n",
    "                               #device=torch.device(\"cuda\")\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that attention and vectorization work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = get_attention(sentences, model, tokenizer, method = 'first')\n",
    "np.sum([tpl[1] for tpl in attentions[1]])\n",
    "\n",
    "vectorized = vectorize(sentences, model, tokenizer)\n",
    "torch.stack(vectorized).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 0.30418563),\n",
       "  ('movie', 0.16935529),\n",
       "  ('was', 0.16058609),\n",
       "  ('extremely', 0.12557101),\n",
       "  ('bad', 0.24030195)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention([\"this movie was extremely bad\"], model, tokenizer, method = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nlwx_2020_hashtags_no_rt_predictions.csv\")\n",
    "data = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "ngram = (1, 3)\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows out of 100.\n"
     ]
    }
   ],
   "source": [
    "negative = negative[:100]\n",
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(data), batch_size):\n",
    "    index = min(i + batch_size, len(data))\n",
    "    rows.append(vectorize(data[i:index], model, tokenizer))\n",
    "    attentions.extend(get_attention(data[i:index], model, tokenizer))\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(data)}.\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = st_model.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions = get_attention(data, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 rows out of 21797.\n",
      "Processed 1000 rows out of 21797.\n",
      "Processed 1500 rows out of 21797.\n",
      "Processed 2000 rows out of 21797.\n",
      "Processed 2500 rows out of 21797.\n",
      "Processed 3000 rows out of 21797.\n",
      "Processed 3500 rows out of 21797.\n",
      "Processed 4000 rows out of 21797.\n",
      "Processed 4500 rows out of 21797.\n",
      "Processed 5000 rows out of 21797.\n",
      "Processed 5500 rows out of 21797.\n",
      "Processed 6000 rows out of 21797.\n",
      "Processed 6500 rows out of 21797.\n",
      "Processed 7000 rows out of 21797.\n",
      "Processed 7500 rows out of 21797.\n",
      "Processed 8000 rows out of 21797.\n",
      "Processed 8500 rows out of 21797.\n",
      "Processed 9000 rows out of 21797.\n",
      "Processed 9500 rows out of 21797.\n",
      "Processed 10000 rows out of 21797.\n",
      "Processed 10500 rows out of 21797.\n",
      "Processed 11000 rows out of 21797.\n",
      "Processed 11500 rows out of 21797.\n",
      "Processed 12000 rows out of 21797.\n",
      "Processed 12500 rows out of 21797.\n",
      "Processed 13000 rows out of 21797.\n",
      "Processed 13500 rows out of 21797.\n",
      "Processed 14000 rows out of 21797.\n",
      "Processed 14500 rows out of 21797.\n",
      "Processed 15000 rows out of 21797.\n",
      "Processed 15500 rows out of 21797.\n",
      "Processed 16000 rows out of 21797.\n",
      "Processed 16500 rows out of 21797.\n",
      "Processed 17000 rows out of 21797.\n",
      "Processed 17500 rows out of 21797.\n",
      "Processed 18000 rows out of 21797.\n",
      "Processed 18500 rows out of 21797.\n",
      "Processed 19000 rows out of 21797.\n",
      "Processed 19500 rows out of 21797.\n",
      "Processed 20000 rows out of 21797.\n",
      "Processed 20500 rows out of 21797.\n",
      "Processed 21000 rows out of 21797.\n",
      "Processed 21500 rows out of 21797.\n"
     ]
    }
   ],
   "source": [
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(data)):\n",
    "    #index = min(i + batch_size, len(data))\n",
    "    rows.extend(st_model.encode([data[i]]))\n",
    "    attentions.extend(get_attention([data[i]], model, tokenizer))\n",
    "    counter += 1\n",
    "    if counter % 500 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(data)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298\n",
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10']\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords-en.json') as fopen:\n",
    "    stopwords = json.load(fopen)\n",
    "print(len(stopwords))\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = np.concatenate(rows, axis = 0)\n",
    "#concat = [item.detach().numpy() for item in concat]\n",
    "concat = np.asarray(concat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = []\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    all_model_data.append((data[i], df.prediction[i], attentions[i], rows[i]))\n",
    "    \n",
    "pickle.dump(all_model_data, open(f\"attentions_sent_embeddings.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = pickle.load(open(\"attentions_sent_embeddings.pkl\", \"rb\"))\n",
    "texts, _, attentions, rows = zip(*all_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting kmeans model.\n",
      "CPU times: user 2.21 s, sys: 596 ms, total: 2.8 s\n",
      "Wall time: 1.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Fitting kmeans model.\")\n",
    "rows = rows[:1000]\n",
    "attentions = attentions[:1000]\n",
    "kmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(rows)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering attentions.\n",
      "CPU times: user 404 ms, sys: 0 ns, total: 404 ms\n",
      "Wall time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "overall, filtered_a = [], []\n",
    "print(\"Filtering attentions.\")\n",
    "for a in attentions:\n",
    "    f = [i for i in a if i[0] not in stopwords]\n",
    "    overall.extend(f)\n",
    "    filtered_a.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ngrams.\n",
      "CPU times: user 108 ms, sys: 0 ns, total: 108 ms\n",
      "Wall time: 108 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Generating ngrams.\")\n",
    "o_ngram = generate_ngram(overall, ngram)\n",
    "features = []\n",
    "for i in o_ngram:\n",
    "    features.append(' '.join([w[0] for w in i]))\n",
    "features = list(set(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining cluster components. This will take awhile. \n",
      "Progress will be printed for every 100th processed property.\n",
      "\n",
      "Processed 100 texts in 1.6 seconds.\n",
      "Processed 200 texts in 3.3 seconds.\n",
      "Processed 300 texts in 5.12 seconds.\n",
      "Processed 400 texts in 6.99 seconds.\n",
      "Processed 500 texts in 8.7 seconds.\n",
      "Processed 600 texts in 10.51 seconds.\n",
      "Processed 700 texts in 12.22 seconds.\n",
      "Processed 800 texts in 13.96 seconds.\n",
      "Processed 900 texts in 15.63 seconds.\n",
      "Processed 1000 texts in 17.39 seconds.\n",
      "Finished determining cluster components. Total time 17.39 seconds.\n",
      "CPU times: user 17.4 s, sys: 20 ms, total: 17.4 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "Determining cluster components. This will take awhile. \n",
    "Progress will be printed for every 100th processed property.\n",
    "\"\"\")\n",
    "\n",
    "components = {}\n",
    "words_label = {}\n",
    "start_time = time.time()\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in components:\n",
    "        components[label] = {}\n",
    "        words_label[label] = []\n",
    "    else:\n",
    "        f = generate_ngram(filtered_a[idx], ngram)\n",
    "        for w in f:\n",
    "            word = ' '.join([r[0] for r in w])\n",
    "            score = np.mean([r[1] for r in w])\n",
    "            if word in features:\n",
    "                if word in components[label]:\n",
    "                    components[label][word] += score\n",
    "                else:\n",
    "                    components[label][word] = score\n",
    "                words_label[label].append(word)\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f'Processed {(idx + 1)} texts in {round(time.time() - start_time, 2)} seconds.')\n",
    "            \n",
    "print(f\"Finished determining cluster components. Total time {round(time.time() - start_time, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#tf_idf_corpus = [[w[0] for w in text] for text in filtered_a]\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n",
    "\n",
    "tf_idf_corpus = [[item for item in words_label[key]] for key in range(0,10)]\n",
    "transformed = tfidf_vectorizer.fit_transform(tf_idf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_value={i[1]:i[0] for i in tfidf_vectorizer.vocabulary_.items()}\n",
    "fully_indexed = []\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fully_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_tfidf_attn = {}\n",
    "components_tfidf = {}\n",
    "for k1 in components:\n",
    "    components_tfidf_attn[k1] = {}\n",
    "    components_tfidf[k1] = {}\n",
    "    for k2 in components[k1]:\n",
    "        components_tfidf_attn[k1][k2] = fully_indexed[k1][k2] * components[k1][k2]\n",
    "        components_tfidf[k1][k2] = fully_indexed[k1][k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_attn = topics_df(\n",
    "    10,\n",
    "    components,\n",
    "    n_words = 10)\n",
    "\n",
    "pickle.dump(topics_attn, open(\"topics_sent_embed.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>.</td>\n",
       "      <td>power</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>stay</td>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snow</td>\n",
       "      <td>.</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>hope</td>\n",
       "      <td>.</td>\n",
       "      <td>snow</td>\n",
       "      <td>#</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>power</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>storm</td>\n",
       "      <td>snowmaggedon2020</td>\n",
       "      <td>.</td>\n",
       "      <td>thinking</td>\n",
       "      <td>#</td>\n",
       "      <td>winds</td>\n",
       "      <td>storm</td>\n",
       "      <td>safe</td>\n",
       "      <td>buried</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#</td>\n",
       "      <td>snow</td>\n",
       "      <td># nlwx</td>\n",
       "      <td>safe</td>\n",
       "      <td>!</td>\n",
       "      <td>storm</td>\n",
       "      <td>snow</td>\n",
       "      <td>!</td>\n",
       "      <td>,</td>\n",
       "      <td>newfoundland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td># snowmaggedon2020</td>\n",
       "      <td>/</td>\n",
       "      <td>stay</td>\n",
       "      <td>,</td>\n",
       "      <td>wind</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>.</td>\n",
       "      <td>outage</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>…</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>nlblizzard2020</td>\n",
       "      <td>#</td>\n",
       "      <td>snow</td>\n",
       "      <td>#</td>\n",
       "      <td>stormageddon2020</td>\n",
       "      <td>. stay</td>\n",
       "      <td>evacuated</td>\n",
       "      <td># newfoundland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hospital</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>https</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>. stay safe</td>\n",
       "      <td>power outage</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>shovel</td>\n",
       "      <td>storm</td>\n",
       "      <td>nlwx</td>\n",
       "      <td>!</td>\n",
       "      <td>house</td>\n",
       "      <td>airport</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>…</td>\n",
       "      <td>[UNK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>labour</td>\n",
       "      <td>!</td>\n",
       "      <td># nlblizzard2020</td>\n",
       "      <td>,</td>\n",
       "      <td>power .</td>\n",
       "      <td>gusts</td>\n",
       "      <td>!</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>trapped</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>… https</td>\n",
       "      <td>,</td>\n",
       "      <td>nlstorm</td>\n",
       "      <td>hoping</td>\n",
       "      <td>. #</td>\n",
       "      <td>. john</td>\n",
       "      <td>wind</td>\n",
       "      <td>#</td>\n",
       "      <td>snow</td>\n",
       "      <td>…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic 0             topic 1           topic 2       topic 3  topic 4  \\\n",
       "0         .                   #                 #             .    power   \n",
       "1      snow                   .      newfoundland          hope        .   \n",
       "2     storm    snowmaggedon2020                 .      thinking        #   \n",
       "3         #                snow            # nlwx          safe        !   \n",
       "4         ,  # snowmaggedon2020                 /          stay        ,   \n",
       "5         …        newfoundland    nlblizzard2020             #     snow   \n",
       "6  hospital            blizzard             https  newfoundland        …   \n",
       "7    shovel               storm              nlwx             !    house   \n",
       "8    labour                   !  # nlblizzard2020             ,  power .   \n",
       "9   … https                   ,           nlstorm        hoping      . #   \n",
       "\n",
       "   topic 5           topic 6       topic 7       topic 8         topic 9  \n",
       "0        .                 .          stay             .               #  \n",
       "1     snow                 #     stay safe         power               .  \n",
       "2    winds             storm          safe        buried               @  \n",
       "3    storm              snow             !             ,    newfoundland  \n",
       "4     wind          blizzard             .        outage               !  \n",
       "5        #  stormageddon2020        . stay     evacuated  # newfoundland  \n",
       "6        …      newfoundland   . stay safe  power outage               ,  \n",
       "7  airport                 ,             ,             …           [UNK]  \n",
       "8    gusts                 !  newfoundland       trapped               '  \n",
       "9   . john              wind             #          snow               …  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>.</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>/</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>:</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>safe</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>https</td>\n",
       "      <td>:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https</td>\n",
       "      <td>: /</td>\n",
       "      <td>https</td>\n",
       "      <td>https</td>\n",
       "      <td>https</td>\n",
       "      <td>/ .</td>\n",
       "      <td>https</td>\n",
       "      <td>safe</td>\n",
       "      <td>https :</td>\n",
       "      <td>https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https :</td>\n",
       "      <td>https</td>\n",
       "      <td>https :</td>\n",
       "      <td>:</td>\n",
       "      <td>https :</td>\n",
       "      <td>https</td>\n",
       "      <td>https :</td>\n",
       "      <td>stay</td>\n",
       "      <td>: /</td>\n",
       "      <td>https :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>: /</td>\n",
       "      <td>https :</td>\n",
       "      <td>: /</td>\n",
       "      <td>https :</td>\n",
       "      <td>: /</td>\n",
       "      <td>https :</td>\n",
       "      <td>: /</td>\n",
       "      <td>:</td>\n",
       "      <td>/ /</td>\n",
       "      <td>: /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/ /</td>\n",
       "      <td>/ /</td>\n",
       "      <td>/ /</td>\n",
       "      <td>: /</td>\n",
       "      <td>/ /</td>\n",
       "      <td>: /</td>\n",
       "      <td>/ /</td>\n",
       "      <td>https</td>\n",
       "      <td>/ .</td>\n",
       "      <td>/ /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/ .</td>\n",
       "      <td>/ .</td>\n",
       "      <td>/ .</td>\n",
       "      <td>/ /</td>\n",
       "      <td>/ .</td>\n",
       "      <td>/ /</td>\n",
       "      <td>/ .</td>\n",
       "      <td>https :</td>\n",
       "      <td>. /</td>\n",
       "      <td>/ .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>. /</td>\n",
       "      <td>. /</td>\n",
       "      <td>. /</td>\n",
       "      <td>/ .</td>\n",
       "      <td>. /</td>\n",
       "      <td>. /</td>\n",
       "      <td>. /</td>\n",
       "      <td>: /</td>\n",
       "      <td>https : /</td>\n",
       "      <td>. /</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic 0  topic 1  topic 2  topic 3  topic 4  topic 5  topic 6    topic 7  \\\n",
       "0        /        /        /        .        /        /        /          /   \n",
       "1        .        #        #        /        .        .        .          .   \n",
       "2        #        .        .        #        #        #        #          #   \n",
       "3        :        :        :     safe        :        :        :  stay safe   \n",
       "4    https      : /    https    https    https      / .    https       safe   \n",
       "5  https :    https  https :        :  https :    https  https :       stay   \n",
       "6      : /  https :      : /  https :      : /  https :      : /          :   \n",
       "7      / /      / /      / /      : /      / /      : /      / /      https   \n",
       "8      / .      / .      / .      / /      / .      / /      / .    https :   \n",
       "9      . /      . /      . /      / .      . /      . /      . /        : /   \n",
       "\n",
       "     topic 8  topic 9  \n",
       "0          /        /  \n",
       "1          .        #  \n",
       "2          :        .  \n",
       "3      https        :  \n",
       "4    https :    https  \n",
       "5        : /  https :  \n",
       "6        / /      : /  \n",
       "7        / .      / /  \n",
       "8        . /      / .  \n",
       "9  https : /      . /  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf = topics_df(\n",
    "    10,\n",
    "    components_tfidf,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>.</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#</td>\n",
       "      <td>.</td>\n",
       "      <td>/</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>stay</td>\n",
       "      <td>power</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>.</td>\n",
       "      <td>safe</td>\n",
       "      <td>power</td>\n",
       "      <td>/</td>\n",
       "      <td>/</td>\n",
       "      <td>safe</td>\n",
       "      <td>…</td>\n",
       "      <td>/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>…</td>\n",
       "      <td>snowmaggedon2020</td>\n",
       "      <td>https</td>\n",
       "      <td>hope</td>\n",
       "      <td>/</td>\n",
       "      <td>:</td>\n",
       "      <td>:</td>\n",
       "      <td>.</td>\n",
       "      <td>/</td>\n",
       "      <td>@</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>,</td>\n",
       "      <td>https</td>\n",
       "      <td>https :</td>\n",
       "      <td>thinking</td>\n",
       "      <td>,</td>\n",
       "      <td>winds</td>\n",
       "      <td>https</td>\n",
       "      <td>!</td>\n",
       "      <td>,</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>… https</td>\n",
       "      <td># snowmaggedon2020</td>\n",
       "      <td>:</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>snow</td>\n",
       "      <td>,</td>\n",
       "      <td>#</td>\n",
       "      <td>:</td>\n",
       "      <td>https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>snow</td>\n",
       "      <td>https :</td>\n",
       "      <td># nlwx</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>…</td>\n",
       "      <td>…</td>\n",
       "      <td>snow</td>\n",
       "      <td>. stay</td>\n",
       "      <td>#</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>… https :</td>\n",
       "      <td>:</td>\n",
       "      <td>https : /</td>\n",
       "      <td>/</td>\n",
       "      <td>https</td>\n",
       "      <td>wind</td>\n",
       "      <td>!</td>\n",
       "      <td>. stay safe</td>\n",
       "      <td>… https</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hospital</td>\n",
       "      <td>https : /</td>\n",
       "      <td>nlwx</td>\n",
       "      <td>,</td>\n",
       "      <td>:</td>\n",
       "      <td>airport</td>\n",
       "      <td>storm</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>… https :</td>\n",
       "      <td>…</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>:</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>. /</td>\n",
       "      <td>stay</td>\n",
       "      <td>… https</td>\n",
       "      <td>… https</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>,</td>\n",
       "      <td>buried</td>\n",
       "      <td>https :</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic 0             topic 1    topic 2       topic 3  topic 4  topic 5  \\\n",
       "0          .                   #          #             .        .        .   \n",
       "1          #                   .          /             #        #        #   \n",
       "2          /                   /          .          safe    power        /   \n",
       "3          …    snowmaggedon2020      https          hope        /        :   \n",
       "4          ,               https    https :      thinking        ,    winds   \n",
       "5    … https  # snowmaggedon2020          :             !        !     snow   \n",
       "6       snow             https :     # nlwx  newfoundland        …        …   \n",
       "7  … https :                   :  https : /             /    https     wind   \n",
       "8   hospital           https : /       nlwx             ,        :  airport   \n",
       "9          :        newfoundland        . /          stay  … https  … https   \n",
       "\n",
       "    topic 6       topic 7    topic 8  topic 9  \n",
       "0         .     stay safe          .        #  \n",
       "1         #          stay      power        .  \n",
       "2         /          safe          …        /  \n",
       "3         :             .          /        @  \n",
       "4     https             !          ,        !  \n",
       "5         ,             #          :    https  \n",
       "6      snow        . stay          #        '  \n",
       "7         !   . stay safe    … https        ,  \n",
       "8     storm  newfoundland  … https :        …  \n",
       "9  blizzard             ,     buried  https :  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf_attn = topics_df(\n",
    "    10,\n",
    "    components_tfidf_attn,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allen NLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
