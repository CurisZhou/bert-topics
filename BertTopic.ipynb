{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from BertForSequenceClassificationOutputPooled import *\n",
    "from BertTM import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import models, losses\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained('bert-base-uncased', \n",
    "                                                              output_attentions=True, \n",
    "                                                              output_hidden_states=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../bert-classifier-pytorch/model_save_attention_1epoch\"\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained(output_dir,\n",
    "                                                      output_attentions = True, \n",
    "                                                      output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = models.BERT(output_dir, max_seq_length = 240,)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "st_model = SentenceTransformer(modules=[word_embedding_model, pooling_model],\n",
    "                               #device=torch.device(\"cuda\")\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that attention and vectorization work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = get_attention(sentences, model, tokenizer, method = 'first')\n",
    "np.sum([tpl[1] for tpl in attentions[1]])\n",
    "\n",
    "vectorized = vectorize(sentences, model, tokenizer)\n",
    "torch.stack(vectorized).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "\n",
    "test = [\"this movie was the cutest. read more at http://worstever.com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "data = data.apply(lambda row: ' '.join([word for word in row.split() \n",
    "                                        if (not word in stopwords) \n",
    "                                        and (not re.match(url_re, word))]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                             vibingðŸ’€ðŸ˜³\n",
       "1                                   Progression storm.\n",
       "2    Snow band starting pivot Avalon. Heavy snow co...\n",
       "3                                                     \n",
       "4    Anytime speculated yesterday severity storm I ...\n",
       "5    @EddieSheerr Eddie... TWN forecast wind starti...\n",
       "6    These pictures crazyy, I complain ottawa winte...\n",
       "7    Visibility reduced ares @towngfw hour. Itâ€™s mo...\n",
       "8    #Hiphops #Lit #dope flow #GotDrip #BING ðŸ’¯ @tik...\n",
       "9                        This bonkers! Snow windows. ðŸ˜®\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 0.0613682),\n",
       "  ('movie', 0.030429687),\n",
       "  ('was', 0.035641603),\n",
       "  ('the', 0.21911351),\n",
       "  ('cutest', 0.06270852),\n",
       "  ('.', 0.09440403),\n",
       "  ('read', 0.0423442),\n",
       "  ('more', 0.06480052),\n",
       "  ('at', 0.06262489),\n",
       "  ('http', 0.027938599),\n",
       "  (':', 0.043154325),\n",
       "  ('/', 0.038011733),\n",
       "  ('/', 0.041409045),\n",
       "  ('worstever', 0.04187157),\n",
       "  ('.', 0.08471608),\n",
       "  ('com', 0.049463503)]]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention([\"this movie was the cutest. read more at http://worstever.com\"], model, tokenizer, method = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nlwx_2020_hashtags_no_rt_predictions.csv\")\n",
    "df = preprocess(df)\n",
    "data = df['preprocessed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "ngram = (1, 3)\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows out of 100.\n"
     ]
    }
   ],
   "source": [
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(data), batch_size):\n",
    "    index = min(i + batch_size, len(data))\n",
    "    rows.append(vectorize(data[i:index], model, tokenizer))\n",
    "    attentions.extend(get_attention(data[i:index], model, tokenizer))\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(data)}.\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 rows out of 20682.\n",
      "Processed 1000 rows out of 20682.\n",
      "Processed 1500 rows out of 20682.\n",
      "Processed 2000 rows out of 20682.\n",
      "Processed 2500 rows out of 20682.\n",
      "Processed 3000 rows out of 20682.\n",
      "Processed 3500 rows out of 20682.\n",
      "Processed 4000 rows out of 20682.\n",
      "Processed 4500 rows out of 20682.\n",
      "Processed 5000 rows out of 20682.\n",
      "Processed 5500 rows out of 20682.\n",
      "Processed 6000 rows out of 20682.\n",
      "Processed 6500 rows out of 20682.\n",
      "Processed 7000 rows out of 20682.\n",
      "Processed 7500 rows out of 20682.\n",
      "Processed 8000 rows out of 20682.\n",
      "Processed 8500 rows out of 20682.\n",
      "Processed 9000 rows out of 20682.\n",
      "Processed 9500 rows out of 20682.\n",
      "Processed 10000 rows out of 20682.\n",
      "Processed 10500 rows out of 20682.\n",
      "Processed 11000 rows out of 20682.\n",
      "Processed 11500 rows out of 20682.\n",
      "Processed 12000 rows out of 20682.\n",
      "Processed 12500 rows out of 20682.\n",
      "Processed 13000 rows out of 20682.\n",
      "Processed 13500 rows out of 20682.\n",
      "Processed 14000 rows out of 20682.\n",
      "Processed 14500 rows out of 20682.\n",
      "Processed 15000 rows out of 20682.\n",
      "Processed 15500 rows out of 20682.\n",
      "Processed 16000 rows out of 20682.\n",
      "Processed 16500 rows out of 20682.\n",
      "Processed 17000 rows out of 20682.\n",
      "Processed 17500 rows out of 20682.\n",
      "Processed 18000 rows out of 20682.\n",
      "Processed 18500 rows out of 20682.\n",
      "Processed 19000 rows out of 20682.\n",
      "Processed 19500 rows out of 20682.\n",
      "Processed 20000 rows out of 20682.\n",
      "Processed 20500 rows out of 20682.\n"
     ]
    }
   ],
   "source": [
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(data)):\n",
    "    rows.extend(st_model.encode([data[i]]))\n",
    "    attentions.extend(get_attention([data[i]], model, tokenizer))\n",
    "    counter += 1\n",
    "    if counter % 500 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(data)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1310\n",
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10']\n"
     ]
    }
   ],
   "source": [
    "with open('stopwords-en.json') as fopen:\n",
    "    stopwords = json.load(fopen)\n",
    "\n",
    "stopwords.extend(['#', '@', 'â€¦', \"'\", \"â€™\", \"[UNK]\", \"\\\"\", \";\", \"*\", \"_\", \"amp\", \"&\"])\n",
    "    \n",
    "print(len(stopwords))\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat = np.concatenate(rows, axis = 0)\n",
    "#concat = [item.detach().numpy() for item in concat]\n",
    "#concat = np.asarray(concat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = []\n",
    "\n",
    "for i in range(len(rows)):\n",
    "    all_model_data.append((data[i], df.prediction[i], attentions[i], rows[i]))\n",
    "    \n",
    "#pickle.dump(all_model_data, open(f\"attentions_sent_embeddings.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_data = pickle.load(open(\"attentions_sent_embeddings.pkl\", \"rb\"))\n",
    "texts, _, attentions, rows = zip(*all_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting kmeans model.\n",
      "CPU times: user 35.7 s, sys: 2.06 s, total: 37.7 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Fitting kmeans model.\")\n",
    "#rows = rows[:1000]\n",
    "#attentions = attentions[:1000]\n",
    "kmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(rows)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering attentions.\n",
      "CPU times: user 7.42 s, sys: 4 ms, total: 7.42 s\n",
      "Wall time: 7.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "overall, filtered_a = [], []\n",
    "url_re = '(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})'\n",
    "print(\"Filtering attentions.\")\n",
    "for a in attentions:\n",
    "    f = [i for i in a if i[0] not in stopwords and i[0] not in url_re]\n",
    "    overall.extend(f)\n",
    "    filtered_a.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating ngrams.\n",
      "CPU times: user 524 ms, sys: 16 ms, total: 540 ms\n",
      "Wall time: 539 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Generating ngrams.\")\n",
    "o_ngram = generate_ngram(overall, ngram)\n",
    "features = []\n",
    "for i in o_ngram:\n",
    "    features.append(' '.join([w[0] for w in i]))\n",
    "features = list(set(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determining cluster components. This will take awhile. \n",
      "Progress will be printed for every 500th processed property.\n",
      "\n",
      "Processed 500 texts in 34.58 seconds.\n",
      "Processed 1000 texts in 69.98 seconds.\n",
      "Processed 1500 texts in 107.74 seconds.\n",
      "Processed 2000 texts in 144.56 seconds.\n",
      "Processed 2500 texts in 184.06 seconds.\n",
      "Processed 3000 texts in 221.92 seconds.\n",
      "Processed 3500 texts in 261.15 seconds.\n",
      "Processed 4500 texts in 334.02 seconds.\n",
      "Processed 5000 texts in 375.48 seconds.\n",
      "Processed 5500 texts in 416.82 seconds.\n",
      "Processed 6000 texts in 457.95 seconds.\n",
      "Processed 6500 texts in 494.66 seconds.\n",
      "Processed 7000 texts in 531.15 seconds.\n",
      "Processed 7500 texts in 565.59 seconds.\n",
      "Processed 8000 texts in 602.03 seconds.\n",
      "Processed 8500 texts in 636.78 seconds.\n",
      "Processed 9000 texts in 670.91 seconds.\n",
      "Processed 9500 texts in 705.01 seconds.\n",
      "Processed 10000 texts in 738.05 seconds.\n",
      "Processed 10500 texts in 774.64 seconds.\n",
      "Processed 11000 texts in 813.0 seconds.\n",
      "Processed 11500 texts in 850.06 seconds.\n",
      "Processed 12000 texts in 883.16 seconds.\n",
      "Processed 12500 texts in 916.47 seconds.\n",
      "Processed 13000 texts in 952.66 seconds.\n",
      "Processed 13500 texts in 996.88 seconds.\n",
      "Processed 14000 texts in 1041.93 seconds.\n",
      "Processed 14500 texts in 1085.92 seconds.\n",
      "Processed 15000 texts in 1128.72 seconds.\n",
      "Processed 15500 texts in 1172.3 seconds.\n",
      "Processed 16000 texts in 1215.77 seconds.\n",
      "Processed 16500 texts in 1256.78 seconds.\n",
      "Processed 17000 texts in 1298.4 seconds.\n",
      "Processed 17500 texts in 1340.09 seconds.\n",
      "Processed 18000 texts in 1379.53 seconds.\n",
      "Processed 18500 texts in 1421.62 seconds.\n",
      "Processed 19000 texts in 1463.85 seconds.\n",
      "Processed 19500 texts in 1501.94 seconds.\n",
      "Processed 20000 texts in 1535.91 seconds.\n",
      "Processed 20500 texts in 1573.27 seconds.\n",
      "Finished determining cluster components. Total time 1586.53 seconds.\n",
      "CPU times: user 26min 25s, sys: 616 ms, total: 26min 26s\n",
      "Wall time: 26min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "print(\n",
    "\"\"\"\n",
    "Determining cluster components. This will take awhile. \n",
    "Progress will be printed for every 500th processed property.\n",
    "\"\"\")\n",
    "\n",
    "components = {}\n",
    "words_label = {}\n",
    "start_time = time.time()\n",
    "for idx, label in enumerate(labels):\n",
    "    if label not in components:\n",
    "        components[label] = {}\n",
    "        words_label[label] = []\n",
    "    else:\n",
    "        f = generate_ngram(filtered_a[idx], ngram)\n",
    "        for w in f:\n",
    "            word = ' '.join([r[0] for r in w])\n",
    "            score = np.mean([r[1] for r in w])\n",
    "            if word in features:\n",
    "                if word in components[label]:\n",
    "                    components[label][word] += score\n",
    "                else:\n",
    "                    components[label][word] = score\n",
    "                words_label[label].append(word)\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f'Processed {(idx + 1)} texts in {round(time.time() - start_time, 2)} seconds.')\n",
    "            \n",
    "print(f\"Finished determining cluster components. Total time {round(time.time() - start_time, 2)} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    tokenizer=dummy_fun,\n",
    "    preprocessor=dummy_fun,\n",
    "    token_pattern=None) \n",
    "\n",
    "tf_idf_corpus = [[item for item in words_label[key]] for key in range(0,10)]\n",
    "transformed = tfidf_vectorizer.fit_transform(tf_idf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_value={i[1]:i[0] for i in tfidf_vectorizer.vocabulary_.items()}\n",
    "fully_indexed = []\n",
    "for row in transformed:\n",
    "    fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(components, open(\"components.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_tfidf_attn = {}\n",
    "components_tfidf = {}\n",
    "for k1 in components:\n",
    "    components_tfidf_attn[k1] = {}\n",
    "    components_tfidf[k1] = {}\n",
    "    for k2 in components[k1]:\n",
    "        components_tfidf_attn[k1][k2] = fully_indexed[k1][k2] * components[k1][k2]\n",
    "        components_tfidf[k1][k2] = fully_indexed[k1][k2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_attn = topics_df(\n",
    "    10,\n",
    "    components,\n",
    "    n_words = 10)\n",
    "\n",
    "pickle.dump(topics_attn, open(\"topics_sent_embed.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assistance</td>\n",
       "      <td>snow</td>\n",
       "      <td>snow</td>\n",
       "      <td>hope</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>snow</td>\n",
       "      <td>stay</td>\n",
       "      <td>emergency</td>\n",
       "      <td>closed</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>support</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>storm</td>\n",
       "      <td>stay</td>\n",
       "      <td>nlstorm</td>\n",
       "      <td>buried</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>snow</td>\n",
       "      <td>power</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>helping</td>\n",
       "      <td>storm</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>safe</td>\n",
       "      <td>nlstorm2020</td>\n",
       "      <td>missing</td>\n",
       "      <td>safe</td>\n",
       "      <td>power</td>\n",
       "      <td>emergency</td>\n",
       "      <td>newfoundland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snow</td>\n",
       "      <td>day</td>\n",
       "      <td>weather</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>digging</td>\n",
       "      <td>warning</td>\n",
       "      <td>closed</td>\n",
       "      <td>snow</td>\n",
       "      <td>snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>people</td>\n",
       "      <td>shovel</td>\n",
       "      <td>emergency</td>\n",
       "      <td>thinking</td>\n",
       "      <td>snow</td>\n",
       "      <td>storm</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>storm</td>\n",
       "      <td>outages</td>\n",
       "      <td>blizzard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>food</td>\n",
       "      <td>people</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "      <td>stuck</td>\n",
       "      <td>advisory</td>\n",
       "      <td>city</td>\n",
       "      <td>storm</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emergency</td>\n",
       "      <td>love</td>\n",
       "      <td>snowfall</td>\n",
       "      <td>hoping</td>\n",
       "      <td>nlsnowstorm2020</td>\n",
       "      <td>trapped</td>\n",
       "      <td>envcanada warning</td>\n",
       "      <td>john</td>\n",
       "      <td>outage</td>\n",
       "      <td>blizzard2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>military</td>\n",
       "      <td>car</td>\n",
       "      <td>winds</td>\n",
       "      <td>friends</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>car</td>\n",
       "      <td>stay safe newfoundland</td>\n",
       "      <td>people</td>\n",
       "      <td>lost</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>supplies</td>\n",
       "      <td>winter</td>\n",
       "      <td>statement weather</td>\n",
       "      <td>god</td>\n",
       "      <td>canada</td>\n",
       "      <td>people</td>\n",
       "      <td>storm</td>\n",
       "      <td>stay</td>\n",
       "      <td>roads</td>\n",
       "      <td>nlstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>assist</td>\n",
       "      <td>time</td>\n",
       "      <td>statement</td>\n",
       "      <td>prayers</td>\n",
       "      <td>love</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>safe newfoundland</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>schools</td>\n",
       "      <td>winter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic 0       topic 1            topic 2       topic 3  \\\n",
       "0  assistance          snow               snow          hope   \n",
       "1     support  newfoundland              storm          stay   \n",
       "2     helping         storm           blizzard          safe   \n",
       "3        snow           day            weather  newfoundland   \n",
       "4      people        shovel          emergency      thinking   \n",
       "5        food        people       newfoundland     stay safe   \n",
       "6   emergency          love           snowfall        hoping   \n",
       "7    military           car              winds       friends   \n",
       "8    supplies        winter  statement weather           god   \n",
       "9      assist          time          statement       prayers   \n",
       "\n",
       "            topic 4   topic 5                 topic 6       topic 7  \\\n",
       "0      newfoundland      snow                    stay     emergency   \n",
       "1           nlstorm    buried               stay safe          snow   \n",
       "2       nlstorm2020   missing                    safe         power   \n",
       "3         nltraffic   digging                 warning        closed   \n",
       "4              snow     storm            newfoundland         storm   \n",
       "5  snowmageddon2020     stuck                advisory          city   \n",
       "6   nlsnowstorm2020   trapped       envcanada warning          john   \n",
       "7         snowstorm       car  stay safe newfoundland        people   \n",
       "8            canada    people                   storm          stay   \n",
       "9              love  blizzard       safe newfoundland  newfoundland   \n",
       "\n",
       "     topic 8           topic 9  \n",
       "0     closed              snow  \n",
       "1      power             storm  \n",
       "2  emergency      newfoundland  \n",
       "3       snow         snowstorm  \n",
       "4    outages          blizzard  \n",
       "5      storm  snowmageddon2020  \n",
       "6     outage      blizzard2020  \n",
       "7       lost           weather  \n",
       "8      roads           nlstorm  \n",
       "9    schools            winter  "
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assistance</td>\n",
       "      <td>snow</td>\n",
       "      <td>snow</td>\n",
       "      <td>safe</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>snow</td>\n",
       "      <td>safe</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>buried</td>\n",
       "      <td>stay</td>\n",
       "      <td>john</td>\n",
       "      <td>john</td>\n",
       "      <td>newfoundland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newfoundland</td>\n",
       "      <td>day</td>\n",
       "      <td>storm</td>\n",
       "      <td>hope</td>\n",
       "      <td>nlstorm2020</td>\n",
       "      <td>car</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>emergency</td>\n",
       "      <td>emergency</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snow</td>\n",
       "      <td>time</td>\n",
       "      <td>john</td>\n",
       "      <td>stay</td>\n",
       "      <td>nlstorm</td>\n",
       "      <td>house</td>\n",
       "      <td>envcanada</td>\n",
       "      <td>road</td>\n",
       "      <td>closed</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food</td>\n",
       "      <td>people</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>eminem</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>warning</td>\n",
       "      <td>power</td>\n",
       "      <td>snow</td>\n",
       "      <td>snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emergency</td>\n",
       "      <td>love</td>\n",
       "      <td>emergency</td>\n",
       "      <td>prayers</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "      <td>john</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>cityofstjohns</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>blizzard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>john</td>\n",
       "      <td>dog</td>\n",
       "      <td>winds</td>\n",
       "      <td>friends</td>\n",
       "      <td>michelleobama</td>\n",
       "      <td>street</td>\n",
       "      <td>envcanada warning</td>\n",
       "      <td>snow</td>\n",
       "      <td>outages</td>\n",
       "      <td>nlstorm2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>support</td>\n",
       "      <td>storm</td>\n",
       "      <td>weather</td>\n",
       "      <td>thinking</td>\n",
       "      <td>saveng</td>\n",
       "      <td>door</td>\n",
       "      <td>safe newfoundland</td>\n",
       "      <td>tomorrow</td>\n",
       "      <td>schools</td>\n",
       "      <td>nlstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>military</td>\n",
       "      <td>morning</td>\n",
       "      <td>nlstorm</td>\n",
       "      <td>warm</td>\n",
       "      <td>ken</td>\n",
       "      <td>people</td>\n",
       "      <td>emergency</td>\n",
       "      <td>city</td>\n",
       "      <td>roads</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>helping</td>\n",
       "      <td>door</td>\n",
       "      <td>30</td>\n",
       "      <td>safe warm</td>\n",
       "      <td>starr</td>\n",
       "      <td>storm</td>\n",
       "      <td>stay safe newfoundland</td>\n",
       "      <td>people</td>\n",
       "      <td>power outages</td>\n",
       "      <td>blizzard2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic 0       topic 1       topic 2       topic 3           topic 4  \\\n",
       "0    assistance          snow          snow          safe      newfoundland   \n",
       "1        people  newfoundland  newfoundland  newfoundland         nltraffic   \n",
       "2  newfoundland           day         storm          hope       nlstorm2020   \n",
       "3          snow          time          john          stay           nlstorm   \n",
       "4          food        people      blizzard     stay safe            eminem   \n",
       "5     emergency          love     emergency       prayers  snowmageddon2020   \n",
       "6          john           dog         winds       friends     michelleobama   \n",
       "7       support         storm       weather      thinking            saveng   \n",
       "8      military       morning       nlstorm          warm               ken   \n",
       "9       helping          door            30     safe warm             starr   \n",
       "\n",
       "        topic 5                 topic 6        topic 7        topic 8  \\\n",
       "0          snow                    safe      nltraffic          power   \n",
       "1        buried                    stay           john           john   \n",
       "2           car               stay safe      emergency      emergency   \n",
       "3         house               envcanada           road         closed   \n",
       "4  newfoundland                 warning          power           snow   \n",
       "5          john            newfoundland  cityofstjohns      nltraffic   \n",
       "6        street       envcanada warning           snow        outages   \n",
       "7          door       safe newfoundland       tomorrow        schools   \n",
       "8        people               emergency           city          roads   \n",
       "9         storm  stay safe newfoundland         people  power outages   \n",
       "\n",
       "            topic 9  \n",
       "0              snow  \n",
       "1      newfoundland  \n",
       "2             storm  \n",
       "3  snowmageddon2020  \n",
       "4         snowstorm  \n",
       "5          blizzard  \n",
       "6       nlstorm2020  \n",
       "7           nlstorm  \n",
       "8               day  \n",
       "9      blizzard2020  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf = topics_df(\n",
    "    10,\n",
    "    components_tfidf,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assistance</td>\n",
       "      <td>snow</td>\n",
       "      <td>snow</td>\n",
       "      <td>safe</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>snow</td>\n",
       "      <td>stay</td>\n",
       "      <td>emergency</td>\n",
       "      <td>closed</td>\n",
       "      <td>snow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>storm</td>\n",
       "      <td>hope</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>buried</td>\n",
       "      <td>safe</td>\n",
       "      <td>power</td>\n",
       "      <td>power</td>\n",
       "      <td>newfoundland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snow</td>\n",
       "      <td>day</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>nlstorm</td>\n",
       "      <td>car</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>snow</td>\n",
       "      <td>emergency</td>\n",
       "      <td>storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>support</td>\n",
       "      <td>storm</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>stay</td>\n",
       "      <td>nlstorm2020</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>warning</td>\n",
       "      <td>john</td>\n",
       "      <td>snow</td>\n",
       "      <td>snowstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food</td>\n",
       "      <td>people</td>\n",
       "      <td>emergency</td>\n",
       "      <td>stay safe</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "      <td>storm</td>\n",
       "      <td>newfoundland</td>\n",
       "      <td>closed</td>\n",
       "      <td>outages</td>\n",
       "      <td>blizzard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>helping</td>\n",
       "      <td>love</td>\n",
       "      <td>weather</td>\n",
       "      <td>thinking</td>\n",
       "      <td>snow</td>\n",
       "      <td>digging</td>\n",
       "      <td>envcanada warning</td>\n",
       "      <td>city</td>\n",
       "      <td>john</td>\n",
       "      <td>snowmageddon2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>emergency</td>\n",
       "      <td>time</td>\n",
       "      <td>winds</td>\n",
       "      <td>friends</td>\n",
       "      <td>nlsnowstorm2020</td>\n",
       "      <td>house</td>\n",
       "      <td>safe newfoundland</td>\n",
       "      <td>road</td>\n",
       "      <td>storm</td>\n",
       "      <td>nlstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>military</td>\n",
       "      <td>shovel</td>\n",
       "      <td>john</td>\n",
       "      <td>hoping</td>\n",
       "      <td>eminem</td>\n",
       "      <td>missing</td>\n",
       "      <td>stay safe newfoundland</td>\n",
       "      <td>people</td>\n",
       "      <td>schools</td>\n",
       "      <td>blizzard2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>newfoundland</td>\n",
       "      <td>dog</td>\n",
       "      <td>snowfall</td>\n",
       "      <td>prayers</td>\n",
       "      <td>canada</td>\n",
       "      <td>stuck</td>\n",
       "      <td>storm</td>\n",
       "      <td>nltraffic</td>\n",
       "      <td>roads</td>\n",
       "      <td>nlstorm2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>supplies</td>\n",
       "      <td>car</td>\n",
       "      <td>wind</td>\n",
       "      <td>warm</td>\n",
       "      <td>love</td>\n",
       "      <td>trapped</td>\n",
       "      <td>emergency</td>\n",
       "      <td>storm</td>\n",
       "      <td>remain</td>\n",
       "      <td>weather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        topic 0       topic 1       topic 2       topic 3           topic 4  \\\n",
       "0    assistance          snow          snow          safe      newfoundland   \n",
       "1        people  newfoundland         storm          hope         nltraffic   \n",
       "2          snow           day  newfoundland  newfoundland           nlstorm   \n",
       "3       support         storm      blizzard          stay       nlstorm2020   \n",
       "4          food        people     emergency     stay safe  snowmageddon2020   \n",
       "5       helping          love       weather      thinking              snow   \n",
       "6     emergency          time         winds       friends   nlsnowstorm2020   \n",
       "7      military        shovel          john        hoping            eminem   \n",
       "8  newfoundland           dog      snowfall       prayers            canada   \n",
       "9      supplies           car          wind          warm              love   \n",
       "\n",
       "        topic 5                 topic 6    topic 7    topic 8  \\\n",
       "0          snow                    stay  emergency     closed   \n",
       "1        buried                    safe      power      power   \n",
       "2           car               stay safe       snow  emergency   \n",
       "3  newfoundland                 warning       john       snow   \n",
       "4         storm            newfoundland     closed    outages   \n",
       "5       digging       envcanada warning       city       john   \n",
       "6         house       safe newfoundland       road      storm   \n",
       "7       missing  stay safe newfoundland     people    schools   \n",
       "8         stuck                   storm  nltraffic      roads   \n",
       "9       trapped               emergency      storm     remain   \n",
       "\n",
       "            topic 9  \n",
       "0              snow  \n",
       "1      newfoundland  \n",
       "2             storm  \n",
       "3         snowstorm  \n",
       "4          blizzard  \n",
       "5  snowmageddon2020  \n",
       "6           nlstorm  \n",
       "7      blizzard2020  \n",
       "8       nlstorm2020  \n",
       "9           weather  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_tfidf_attn = topics_df(\n",
    "    10,\n",
    "    components_tfidf_attn,\n",
    "    n_words = 10)\n",
    "\n",
    "topics_tfidf_attn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allen NLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
