{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from BertForSequenceClassificationOutputPooled import *\n",
    "from BertTM import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained('bert-base-uncased', \n",
    "                                                              output_attentions=True, \n",
    "                                                              output_hidden_states=True)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"model_save_attention_1epoch\"\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassificationOutputPooled.from_pretrained(output_dir,\n",
    "                                                      output_attentions = True, \n",
    "                                                      output_hidden_states = True)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "labels = torch.tensor([1]).unsqueeze(0)\n",
    "input_list = []\n",
    "token_list = []\n",
    "cls_ = '[CLS]'\n",
    "sep_ = '[SEP]'\n",
    "sentences = ['Hello, my dog is cute and cutest.', 'I am too']\n",
    "for i, sent in enumerate(sentences):\n",
    "    inputs = tokenizer.encode_plus(sentences[i], add_special_tokens=True)\n",
    "    tokens = [cls_] + tokenizer.tokenize(sentences[i]) + [sep_]\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0)\n",
    "    input_list.append(input_ids)\n",
    "    token_list.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that attention and vectorization work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions = get_attention(sentences, model, tokenizer, method = 'first')\n",
    "np.sum([tpl[1] for tpl in attentions[1]])\n",
    "\n",
    "vectorized = vectorize(sentences, model, tokenizer)\n",
    "torch.stack(vectorized).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('this', 0.30418563),\n",
       "  ('movie', 0.16935529),\n",
       "  ('was', 0.16058609),\n",
       "  ('extremely', 0.12557101),\n",
       "  ('bad', 0.24030195)]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attention([\"this movie was extremely bad\"], model, tokenizer, method = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5330"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!wget https://raw.githubusercontent.com/huseinzol05/NLP-Models-Tensorflow/master/text-classification/data/negative/negative\n",
    "\n",
    "with open('negative') as fopen:\n",
    "    negative = fopen.read().split('\\n')[:-1]\n",
    "len(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "ngram = (1, 3)\n",
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 rows out of 100.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "\n",
    "negative = negative[:100]\n",
    "rows, attentions = [], []\n",
    "counter = 0\n",
    "for i in range(0, len(negative), batch_size):\n",
    "    index = min(i + batch_size, len(negative))\n",
    "    rows.append(vectorize(negative[i:index], model, tokenizer))\n",
    "    attentions.extend(get_attention(negative[i:index], model, tokenizer))\n",
    "    if counter % 50 == 0:\n",
    "        print(f\"Processed {counter} rows out of {len(negative)}.\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298\n",
      "[\"'ll\", \"'tis\", \"'twas\", \"'ve\", '10']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('stopwords-en.json') as fopen:\n",
    "    stopwords = json.load(fopen)\n",
    "print(len(stopwords))\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram(seq, ngram = (1, 3)):\n",
    "    g = []\n",
    "    for i in range(ngram[0], ngram[-1] + 1):\n",
    "        g.extend(list(ngrams_generator(seq, i)))\n",
    "    return g\n",
    "\n",
    "def _pad_sequence(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    sequence = iter(sequence)\n",
    "    if pad_left:\n",
    "        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
    "    if pad_right:\n",
    "        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def ngrams_generator(\n",
    "    sequence,\n",
    "    n,\n",
    "    pad_left = False,\n",
    "    pad_right = False,\n",
    "    left_pad_symbol = None,\n",
    "    right_pad_symbol = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    generate ngrams.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequence : list of str\n",
    "        list of tokenize words.\n",
    "    n : int\n",
    "        ngram size\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ngram: list\n",
    "    \"\"\"\n",
    "    sequence = _pad_sequence(\n",
    "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
    "    )\n",
    "\n",
    "    history = []\n",
    "    while n > 1:\n",
    "        try:\n",
    "            next_item = next(sequence)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        history.append(next_item)\n",
    "        n -= 1\n",
    "    for item in sequence:\n",
    "        history.append(item)\n",
    "        yield tuple(history)\n",
    "        del history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = np.concatenate(rows, axis = 0)\n",
    "concat = [item.detach().numpy() for item in concat]\n",
    "concat = np.asarray(concat, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(concat)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "overall, filtered_a = [], []\n",
    "for a in attentions:\n",
    "    f = [i for i in a if i[0] not in stopwords]\n",
    "    overall.extend(f)\n",
    "    filtered_a.append(f)\n",
    "\n",
    "o_ngram = generate_ngram(overall, ngram)\n",
    "features = []\n",
    "for i in o_ngram:\n",
    "    features.append(' '.join([w[0] for w in i]))\n",
    "features = list(set(features))\n",
    "\n",
    "components = np.zeros((n_topics, len(features)))\n",
    "for no, i in enumerate(labels):\n",
    "    if (no + 1) % 500 == 0:\n",
    "        print('processed %d'%(no + 1))\n",
    "    f = generate_ngram(filtered_a[no], ngram)\n",
    "    for w in f:\n",
    "        word = ' '.join([r[0] for r in w])\n",
    "        score = np.mean([r[1] for r in w])\n",
    "        if word in features:\n",
    "            components[i, features.index(word)] += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics_modelling(\n",
    "    topics, feature_names, sorting, n_words = 20, return_df = True\n",
    "):\n",
    "    if return_df:\n",
    "        try:\n",
    "            import pandas as pd\n",
    "        except:\n",
    "            raise Exception(\n",
    "                'pandas not installed. Please install it and try again or set `return_df = False`'\n",
    "            )\n",
    "    df = {}\n",
    "    for i in range(topics):\n",
    "        words = []\n",
    "        for k in range(n_words):\n",
    "            words.append(feature_names[sorting[i, k]])\n",
    "        df['topic %d' % (i)] = words\n",
    "    if return_df:\n",
    "        return pd.DataFrame.from_dict(df)\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic 0</th>\n",
       "      <th>topic 1</th>\n",
       "      <th>topic 2</th>\n",
       "      <th>topic 3</th>\n",
       "      <th>topic 4</th>\n",
       "      <th>topic 5</th>\n",
       "      <th>topic 6</th>\n",
       "      <th>topic 7</th>\n",
       "      <th>topic 8</th>\n",
       "      <th>topic 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analyze</td>\n",
       "      <td>compelling</td>\n",
       "      <td>silly</td>\n",
       "      <td>overboard</td>\n",
       "      <td>movie</td>\n",
       "      <td>characters</td>\n",
       "      <td>watch</td>\n",
       "      <td>subplots</td>\n",
       "      <td>owes</td>\n",
       "      <td>time impossible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comedy</td>\n",
       "      <td>earnest</td>\n",
       "      <td>silly tedious</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>film</td>\n",
       "      <td>watch annoying</td>\n",
       "      <td>examination</td>\n",
       "      <td>sonnenfeld owes</td>\n",
       "      <td>guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>romantic comedy</td>\n",
       "      <td>earnest heavyhanded</td>\n",
       "      <td>dull</td>\n",
       "      <td>film overboard</td>\n",
       "      <td>coma</td>\n",
       "      <td>script</td>\n",
       "      <td>character</td>\n",
       "      <td>watching</td>\n",
       "      <td>owes frank</td>\n",
       "      <td>silly tedious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie</td>\n",
       "      <td>heavyhanded</td>\n",
       "      <td>simplistic silly</td>\n",
       "      <td>dominates</td>\n",
       "      <td>hey</td>\n",
       "      <td>action</td>\n",
       "      <td>easy watch</td>\n",
       "      <td>moments subplots</td>\n",
       "      <td>cameo</td>\n",
       "      <td>farce parody comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romantic</td>\n",
       "      <td>sour</td>\n",
       "      <td>simplistic silly tedious</td>\n",
       "      <td>prison</td>\n",
       "      <td>sports</td>\n",
       "      <td>feels</td>\n",
       "      <td>easy watch annoying</td>\n",
       "      <td>woman</td>\n",
       "      <td>barry sonnenfeld owes</td>\n",
       "      <td>gems field roughage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inconsequential romantic comedy</td>\n",
       "      <td>film</td>\n",
       "      <td>tedious</td>\n",
       "      <td>seagal</td>\n",
       "      <td>sports movie</td>\n",
       "      <td>crowd</td>\n",
       "      <td>lead character</td>\n",
       "      <td>moments</td>\n",
       "      <td>sonnenfeld owes frank</td>\n",
       "      <td>report totalitarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>odd</td>\n",
       "      <td>survived</td>\n",
       "      <td>gloom</td>\n",
       "      <td>rollerball film overboard</td>\n",
       "      <td>worse</td>\n",
       "      <td>action clichs</td>\n",
       "      <td>watch annoying demeanour</td>\n",
       "      <td>robotic</td>\n",
       "      <td>owes frank pug</td>\n",
       "      <td>sundance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ceos</td>\n",
       "      <td>farcical sour</td>\n",
       "      <td>simplistic</td>\n",
       "      <td>koolaid</td>\n",
       "      <td>american sports movie</td>\n",
       "      <td>mixed</td>\n",
       "      <td>annoying</td>\n",
       "      <td>movie</td>\n",
       "      <td>movie</td>\n",
       "      <td>covers huge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>film</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>kinda</td>\n",
       "      <td>orange prison</td>\n",
       "      <td>pc</td>\n",
       "      <td>gags</td>\n",
       "      <td>lead</td>\n",
       "      <td>funny</td>\n",
       "      <td>story</td>\n",
       "      <td>bullets hit sascha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spiritual</td>\n",
       "      <td>code</td>\n",
       "      <td>hotel</td>\n",
       "      <td>field</td>\n",
       "      <td>relative</td>\n",
       "      <td>feels cold</td>\n",
       "      <td>demeanour lead character</td>\n",
       "      <td>freeman</td>\n",
       "      <td>baboon cameo</td>\n",
       "      <td>haunted house</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           topic 0              topic 1  \\\n",
       "0                          analyze           compelling   \n",
       "1                           comedy              earnest   \n",
       "2                  romantic comedy  earnest heavyhanded   \n",
       "3                            movie          heavyhanded   \n",
       "4                         romantic                 sour   \n",
       "5  inconsequential romantic comedy                 film   \n",
       "6                              odd             survived   \n",
       "7                             ceos        farcical sour   \n",
       "8                             film          sentimental   \n",
       "9                        spiritual                 code   \n",
       "\n",
       "                    topic 2                    topic 3                topic 4  \\\n",
       "0                     silly                  overboard                  movie   \n",
       "1             silly tedious                       film                   film   \n",
       "2                      dull             film overboard                   coma   \n",
       "3          simplistic silly                  dominates                    hey   \n",
       "4  simplistic silly tedious                     prison                 sports   \n",
       "5                   tedious                     seagal           sports movie   \n",
       "6                     gloom  rollerball film overboard                  worse   \n",
       "7                simplistic                    koolaid  american sports movie   \n",
       "8                     kinda              orange prison                     pc   \n",
       "9                     hotel                      field               relative   \n",
       "\n",
       "         topic 5                   topic 6           topic 7  \\\n",
       "0     characters                     watch          subplots   \n",
       "1           film            watch annoying       examination   \n",
       "2         script                 character          watching   \n",
       "3         action                easy watch  moments subplots   \n",
       "4          feels       easy watch annoying             woman   \n",
       "5          crowd            lead character           moments   \n",
       "6  action clichs  watch annoying demeanour           robotic   \n",
       "7          mixed                  annoying             movie   \n",
       "8           gags                      lead             funny   \n",
       "9     feels cold  demeanour lead character           freeman   \n",
       "\n",
       "                 topic 8              topic 9  \n",
       "0                   owes      time impossible  \n",
       "1        sonnenfeld owes                  guy  \n",
       "2             owes frank        silly tedious  \n",
       "3                  cameo  farce parody comedy  \n",
       "4  barry sonnenfeld owes  gems field roughage  \n",
       "5  sonnenfeld owes frank  report totalitarian  \n",
       "6         owes frank pug             sundance  \n",
       "7                  movie          covers huge  \n",
       "8                  story   bullets hit sascha  \n",
       "9           baboon cameo        haunted house  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_topics_modelling(\n",
    "    10,\n",
    "    feature_names = np.array(features),\n",
    "    sorting = np.argsort(components)[:, ::-1],\n",
    "    n_words = 10,\n",
    "    return_df = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Allen NLP",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
